# üöÄ DeepSeek-Inference: Local AI Deployment

Run **DeepSeek LLM models** (7B/67B) locally with GPU/CPU support. Includes ready-to-use inference scripts and API examples.

![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python)
![License](https://img.shields.io/badge/License-MIT-green)
![Platform](https://img.shields.io/badge/OS-Windows%20|%20Linux%20|%20macOS-lightgrey)

## Features
‚úÖ **Local inference** for `deepseek-llm-7b` and other variants  
‚úÖ **GPU (CUDA) and CPU (GGUF) support**  
‚úÖ **Simple CLI and API examples**  
‚úÖ MIT Licensed - Free for commercial use  

---

## üõ†Ô∏è Quick Start

### Prerequisites
- Python 3.8+
- NVIDIA GPU (Optional, for CUDA acceleration)
- [Git](https://git-scm.com/)

### 1. Clone the Repository
```bash
git clone https://github.com/akaf47/DeepSeek-Inference.git
cd DeepSeek-Inference



### 2. Install Dependencies
# Create and activate virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/macOS
.\venv\Scripts\activate   # Windows

# Install requirements
pip install -r requirements.txt